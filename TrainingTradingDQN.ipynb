{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Trading Recommendation Model (DQN)\n"
      ],
      "metadata": {
        "id": "Wx017MmA-eG5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Environment"
      ],
      "metadata": {
        "id": "DiSGl5y5We2K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# from data_manager import download_stock_data, preprocess_data, save_data\n",
        "\n",
        "class StockTradingEnv:\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "        self.n_step = len(self.data)\n",
        "        self.current_step = None\n",
        "        self.cash_in_hand = None\n",
        "        self.shares_held = None\n",
        "        self.current_stock_price = None\n",
        "        self.total_shares_owned = 0\n",
        "        self.initial_investment = 10000\n",
        "        self.action_space = 3\n",
        "        self.state_size = 4\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.current_step = 0\n",
        "        self.cash_in_hand = self.initial_investment\n",
        "        self.shares_held = 0\n",
        "        self.total_shares_owned = 0\n",
        "        self.current_stock_price = self.data['Close'].iloc[self.current_step]\n",
        "        return self._get_state()\n",
        "\n",
        "    def step(self, action):\n",
        "        self.current_stock_price = self.data['Close'].iloc[self.current_step]\n",
        "        prev_val = self._get_val()\n",
        "\n",
        "        if action == 0:\n",
        "            self._buy_stock()\n",
        "        elif action == 1:\n",
        "            self._sell_stock()\n",
        "\n",
        "        self.current_step += 1\n",
        "        if self.current_step >= len(self.data):\n",
        "            done = True\n",
        "            self.current_step = 0\n",
        "        else:\n",
        "            done = False\n",
        "\n",
        "        cur_val = self._get_val()\n",
        "        reward = cur_val - prev_val\n",
        "        info = {'current_portfolio_value': cur_val}\n",
        "\n",
        "        print(f\"Step: {self.current_step}, Action: {action}, Done: {done}, Portfolio Value: {cur_val}, Reward: {reward}\")\n",
        "\n",
        "        return self._get_state(), reward, done, info\n",
        "\n",
        "    def _get_state(self):\n",
        "        return np.array([self.current_stock_price, self.shares_held, self.cash_in_hand, self.total_shares_owned])\n",
        "\n",
        "    def _get_val(self):\n",
        "        return self.shares_held * self.current_stock_price + self.cash_in_hand\n",
        "\n",
        "    def _buy_stock(self):\n",
        "        if self.cash_in_hand >= self.current_stock_price:\n",
        "            self.shares_held += 1\n",
        "            self.cash_in_hand -= self.current_stock_price\n",
        "            self.total_shares_owned += 1\n",
        "\n",
        "    def _sell_stock(self):\n",
        "        if self.shares_held > 0:\n",
        "            self.shares_held -= 1\n",
        "            self.cash_in_hand += self.current_stock_price\n",
        "            self.total_shares_owned = max(0, self.total_shares_owned - 1)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bXhM8r2GWjx8"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model"
      ],
      "metadata": {
        "id": "Y71zMmTwWZfU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "8NrekPC2WTtd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from collections import deque\n",
        "from tensorflow.keras import models, layers, optimizers\n",
        "\n",
        "class DQN:\n",
        "    def __init__(self, state_size, action_size):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=2000)\n",
        "        self.gamma = 0.99  # Higher discount rate for long-term profit\n",
        "        self.epsilon = 1.0  # Initial exploration rate\n",
        "        self.epsilon_min = 0.05  # Reduce the minimum exploration rate\n",
        "        self.epsilon_decay = 0.997  # Slower decay for better exploration-exploitation balance\n",
        "        self.target_update_interval = 10  # Update the target model every 10 steps\n",
        "        self.target_model = self._build_model()\n",
        "        self.model = self._build_model()\n",
        "        self.target_model.set_weights(self.model.get_weights())\n",
        "        self.target_update_counter = 0\n",
        "\n",
        "    def _build_model(self):\n",
        "        \"\"\"Neural network for Deep Q-learning model.\"\"\"\n",
        "        model = models.Sequential()\n",
        "        model.add(layers.Dense(48, activation='relu', input_shape=(self.state_size,)))\n",
        "        model.add(layers.Dense(48, activation='relu'))\n",
        "        model.add(layers.Dense(self.action_size, activation='linear'))\n",
        "        model.compile(loss='mse', optimizer=optimizers.Adam(learning_rate=0.0005))\n",
        "        return model\n",
        "\n",
        "    def update_target_model(self):\n",
        "        \"\"\"Update the target model to match the main model.\"\"\"\n",
        "        self.target_model.set_weights(self.model.get_weights())\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def act(self, state):\n",
        "        \"\"\"Select an action based on the state (epsilon-greedy policy).\"\"\"\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return random.randrange(self.action_size)\n",
        "        if len(state.shape) == 1:\n",
        "            state = np.expand_dims(state, axis=0)\n",
        "        act_values = self.model.predict(state, verbose=0)\n",
        "        return np.argmax(act_values[0])\n",
        "\n",
        "    def replay(self, batch_size):\n",
        "        \"\"\"Replay function for full batch processing and loss reporting.\"\"\"\n",
        "        if len(self.memory) < batch_size:\n",
        "            print(\"Not enough samples for replay.\")\n",
        "            return None\n",
        "\n",
        "        minibatch = random.sample(self.memory, batch_size)\n",
        "        losses = []\n",
        "\n",
        "        for state, action, reward, next_state, done in minibatch:\n",
        "            target = reward\n",
        "            if not done:\n",
        "                q_next = self.target_model.predict(next_state, verbose=0)[0]\n",
        "                target += self.gamma * np.amax(q_next)\n",
        "\n",
        "            target_f = self.model.predict(state, verbose=0)\n",
        "            target_f[0][action] = target\n",
        "\n",
        "            loss = self.model.fit(state, target_f, epochs=1, verbose=0).history['loss'][0]\n",
        "            losses.append(loss)\n",
        "\n",
        "        avg_loss = np.mean(losses) if losses else None\n",
        "        print(f\"Average batch loss: {avg_loss}\")\n",
        "\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "        self.target_update_counter += 1\n",
        "        if self.target_update_counter % self.target_update_interval == 0:\n",
        "            self.update_target_model()\n",
        "\n",
        "        return avg_loss\n",
        "\n",
        "    def load(self, name):\n",
        "        \"\"\"Load the entire model from a file.\"\"\"\n",
        "        self.model = models.load_model(name)\n",
        "        self.target_model.set_weights(self.model.get_weights())\n",
        "\n",
        "    def save(self, name):\n",
        "        \"\"\"Save the entire model to a file.\"\"\"\n",
        "        self.model.save(name)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train"
      ],
      "metadata": {
        "id": "WfqhyjDDWpcO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYC37jKPXwnj",
        "outputId": "70c1ba14-675c-4870-95f0-eb3ffd8f7391"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**UTLILTY FUNCTIONS**"
      ],
      "metadata": {
        "id": "m4YbFyyGZEfC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_dqn(val_env, dqn, batch_size):\n",
        "    \"\"\"Evaluate the DQN model on validation data and calculate validation accuracy.\"\"\"\n",
        "    print(\"Starting validation...\")\n",
        "    state = val_env.reset()\n",
        "    state = np.reshape(state, [1, val_env.state_size])\n",
        "    total_loss = []\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "    while True:\n",
        "        action = dqn.act(state)\n",
        "        next_state, reward, done, _ = val_env.step(action)\n",
        "        next_state = np.reshape(next_state, [1, val_env.state_size])\n",
        "\n",
        "        # Check if the action taken was profitable\n",
        "        correct_predictions += reward > 0\n",
        "        total_predictions += 1\n",
        "\n",
        "        # Calculate loss without fitting\n",
        "        target = reward\n",
        "        if not done:\n",
        "            q_next = dqn.model.predict(next_state, verbose=0)[0]\n",
        "            target += dqn.gamma * np.amax(q_next)\n",
        "\n",
        "        target_f = dqn.model.predict(state, verbose=0)\n",
        "        target_f[0][action] = target\n",
        "\n",
        "        # Calculate loss manually without fitting the model\n",
        "        loss = np.mean((target_f - state) ** 2)\n",
        "        total_loss.append(loss)\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "        state = next_state\n",
        "\n",
        "    # Calculate accuracy as the ratio of profitable actions to total actions\n",
        "    validation_accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
        "    avg_loss = np.mean(total_loss) if total_loss else None\n",
        "\n",
        "    print(f\"Validation completed. Accuracy: {validation_accuracy}, Loss: {avg_loss}\")\n",
        "    return avg_loss, validation_accuracy\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def evaluate_on_test_set(test_data, dqn):\n",
        "    \"\"\"Evaluate the DQN model's final performance on the test set.\"\"\"\n",
        "    test_env = StockTradingEnv(data=test_data)\n",
        "    state = test_env.reset()\n",
        "    state = np.reshape(state, [1, test_env.state_size])\n",
        "\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "    total_loss = []\n",
        "\n",
        "    while True:\n",
        "        action = dqn.act(state)\n",
        "        next_state, reward, done, _ = test_env.step(action)\n",
        "        next_state = np.reshape(next_state, [1, test_env.state_size])\n",
        "\n",
        "        # Check if the action was profitable\n",
        "        correct_predictions += reward > 0\n",
        "        total_predictions += 1\n",
        "\n",
        "        # Compute the loss\n",
        "        target = reward\n",
        "        if not done:\n",
        "            q_next = dqn.model.predict(next_state, verbose=0)[0]\n",
        "            target += dqn.gamma * np.amax(q_next)\n",
        "\n",
        "        target_f = dqn.model.predict(state, verbose=0)\n",
        "        target_f[0][action] = target\n",
        "\n",
        "        loss = dqn.model.fit(state, target_f, epochs=1, verbose=0).history['loss'][0]\n",
        "        total_loss.append(loss)\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "        state = next_state\n",
        "\n",
        "    test_accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
        "    return np.mean(total_loss), test_accuracy\n"
      ],
      "metadata": {
        "id": "KtOUSbEyZLK-"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def aggregate_and_split_half(directory, test_size=0.2, validation_size=0.2):\n",
        "    files = [os.path.join(directory, file) for file in os.listdir(directory) if file.endswith('.csv')]\n",
        "    all_data = pd.DataFrame()\n",
        "    for file in files:\n",
        "        df = pd.read_csv(file)\n",
        "        all_data = pd.concat([all_data, df], ignore_index=True)\n",
        "\n",
        "    # Randomly sample half of the dataset\n",
        "    sampled_data = all_data.sample(frac=0.5, random_state=42)\n",
        "\n",
        "    train_val_data, test_data = train_test_split(sampled_data, test_size=test_size, shuffle=True)\n",
        "    train_data, val_data = train_test_split(train_val_data, test_size=validation_size, shuffle=True)\n",
        "\n",
        "    print(f\"Training data shape: {train_data.shape}\")\n",
        "    print(f\"Validation data shape: {val_data.shape}\")\n",
        "    print(f\"Test data shape: {test_data.shape}\")\n",
        "\n",
        "    return train_data, val_data, test_data\n",
        "\n",
        "def compute_reward(portfolio_value, previous_value):\n",
        "    if portfolio_value > previous_value:\n",
        "        return (portfolio_value - previous_value) * 2  # Double the reward for profitable actions\n",
        "    else:\n",
        "        return (portfolio_value - previous_value) - 1  # Penalize poor trades more heavily\n",
        "\n",
        "# Modify the training function to include the new DQN structure and reward function\n",
        "def train_dqn(episodes, batch_size, directory):\n",
        "    train_data, val_data, test_data = aggregate_and_split_data(directory)\n",
        "    train_env = StockTradingEnv(data=train_data)\n",
        "    val_env = StockTradingEnv(data=val_data)\n",
        "    state_size = train_env.state_size\n",
        "    action_size = 3\n",
        "\n",
        "    dqn = DQN(state_size, action_size)\n",
        "\n",
        "    total_rewards = []\n",
        "    episode_losses = []\n",
        "    best_reward = -float('inf')\n",
        "    save_path = '/content/drive/My Drive/best_dqn_model.h5'\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        state = train_env.reset()\n",
        "        state = np.reshape(state, [1, state_size])\n",
        "        total_reward = 0\n",
        "        episode_loss = []\n",
        "\n",
        "        while True:\n",
        "            action = dqn.act(state)\n",
        "            next_state, reward, done, info = train_env.step(action)\n",
        "            reward = compute_reward(info['current_portfolio_value'], train_env._get_val())\n",
        "            next_state = np.reshape(next_state, [1, state_size])\n",
        "\n",
        "            dqn.remember(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "\n",
        "            if done:\n",
        "                average_loss = np.mean(episode_loss) if episode_loss else None\n",
        "                total_rewards.append(total_reward)\n",
        "                episode_losses.append(average_loss)\n",
        "\n",
        "                print(f\"Episode {episode + 1}/{episodes} finished with Reward: {total_reward}, \"\n",
        "                      f\"Avg Loss: {average_loss}, Portfolio Value: {info['current_portfolio_value']}\")\n",
        "                break\n",
        "\n",
        "            if len(dqn.memory) > batch_size:\n",
        "                loss = dqn.replay(batch_size)\n",
        "                if loss is not None:\n",
        "                    episode_loss.append(loss)\n",
        "\n",
        "        val_loss, val_accuracy = validate_dqn(val_env, dqn, batch_size)\n",
        "        print(f\"Validation Loss after Episode {episode + 1}: {val_loss}, \"\n",
        "              f\"Validation Accuracy: {val_accuracy}\")\n",
        "\n",
        "        if total_reward > best_reward:\n",
        "            best_reward = total_reward\n",
        "            dqn.save(save_path)\n",
        "            print(f\"New best model saved with Reward: {best_reward}\")\n",
        "\n",
        "    print(\"Evaluating on Test Set\")\n",
        "    test_loss, test_accuracy = evaluate_on_test_set(test_data, dqn)\n",
        "    print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")\n",
        "\n",
        "    print(\"Training completed.\")\n",
        "    print(f\"Final Avg Reward: {np.mean(total_rewards)}\")\n",
        "    print(f\"Final Avg Loss: {np.mean([l for l in episode_losses if l is not None])}\")\n",
        "\n",
        "# Set the directory where 'historical_data' is stored\n",
        "historical_data_directory = '/content/drive/MyDrive/historical_data'\n",
        "\n",
        "# Train the DQN model with the specified episodes and batch size\n",
        "train_dqn_half_data(1000, 16, historical_data_directory)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "oiDWS1JlWhw6",
        "outputId": "e8ace1f6-fd53-4fd2-957e-4308fb1cc359"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data shape: (17116, 8)\n",
            "Validation data shape: (4279, 8)\n",
            "Test data shape: (5349, 8)\n",
            "Step: 1, Action: 0, Done: False, Portfolio Value: 10000.0, Reward: 0.0\n",
            "Step: 2, Action: 1, Done: False, Portfolio Value: 9999.883790447473, Reward: 0.0\n",
            "Step: 3, Action: 0, Done: False, Portfolio Value: 9999.883790447473, Reward: 0.0\n",
            "Step: 4, Action: 1, Done: False, Portfolio Value: 9999.846514869992, Reward: 0.0\n",
            "Step: 5, Action: 0, Done: False, Portfolio Value: 9999.846514869992, Reward: 0.0\n",
            "Step: 6, Action: 1, Done: False, Portfolio Value: 9999.613369138857, Reward: 0.0\n",
            "Step: 7, Action: 2, Done: False, Portfolio Value: 9999.613369138857, Reward: 0.0\n",
            "Step: 8, Action: 2, Done: False, Portfolio Value: 9999.613369138857, Reward: 0.0\n",
            "Step: 9, Action: 2, Done: False, Portfolio Value: 9999.613369138857, Reward: 0.0\n",
            "Step: 10, Action: 2, Done: False, Portfolio Value: 9999.613369138857, Reward: 0.0\n",
            "Step: 11, Action: 0, Done: False, Portfolio Value: 9999.613369138857, Reward: 0.0\n",
            "Step: 12, Action: 2, Done: False, Portfolio Value: 9999.28997287184, Reward: 0.0\n",
            "Step: 13, Action: 0, Done: False, Portfolio Value: 9999.178457483662, Reward: 0.0\n",
            "Step: 14, Action: 1, Done: False, Portfolio Value: 9999.09445852689, Reward: 1.8189894035458565e-12\n",
            "Step: 15, Action: 2, Done: False, Portfolio Value: 9999.242034364532, Reward: 0.0\n",
            "Step: 16, Action: 1, Done: False, Portfolio Value: 9999.232295330017, Reward: 0.0\n",
            "Step: 17, Action: 1, Done: False, Portfolio Value: 9999.232295330017, Reward: 0.0\n",
            "Average batch loss: 243771.75873303413\n",
            "Step: 18, Action: 0, Done: False, Portfolio Value: 9999.232295330017, Reward: 0.0\n",
            "Average batch loss: 76312.24438476562\n",
            "Step: 19, Action: 2, Done: False, Portfolio Value: 9999.58783932236, Reward: 0.0\n",
            "Average batch loss: 32055.691436767578\n",
            "Step: 20, Action: 2, Done: False, Portfolio Value: 9999.327287677694, Reward: 0.0\n",
            "Average batch loss: 8849.099931716919\n",
            "Step: 21, Action: 1, Done: False, Portfolio Value: 9999.241920176668, Reward: 0.0\n",
            "Average batch loss: 2620.0437470674515\n",
            "Step: 22, Action: 0, Done: False, Portfolio Value: 9999.241920176668, Reward: 0.0\n",
            "Average batch loss: 791.3731147348881\n",
            "Step: 23, Action: 0, Done: False, Portfolio Value: 9999.327386592184, Reward: 0.0\n",
            "Average batch loss: 182.6781313419342\n",
            "Step: 24, Action: 2, Done: False, Portfolio Value: 9999.143852773132, Reward: 0.0\n",
            "Average batch loss: 42.24478417634964\n",
            "Step: 25, Action: 0, Done: False, Portfolio Value: 10000.962366619768, Reward: 0.0\n",
            "Average batch loss: 7.171718833036721\n",
            "Step: 26, Action: 1, Done: False, Portfolio Value: 9998.26209765825, Reward: 0.0\n",
            "Average batch loss: 0.6081676037629222\n",
            "Step: 27, Action: 2, Done: False, Portfolio Value: 9999.30466165468, Reward: 0.0\n",
            "Average batch loss: 8.372562505384849\n",
            "Step: 28, Action: 1, Done: False, Portfolio Value: 9998.44193977052, Reward: 1.8189894035458565e-12\n",
            "Average batch loss: 1.2589006598573178\n",
            "Step: 29, Action: 1, Done: False, Portfolio Value: 9999.060576629692, Reward: 0.0\n",
            "Average batch loss: 0.39158193728144397\n",
            "Step: 30, Action: 1, Done: False, Portfolio Value: 9999.060576629692, Reward: 0.0\n",
            "Average batch loss: 0.05473821682880953\n",
            "Step: 31, Action: 2, Done: False, Portfolio Value: 9999.060576629692, Reward: 0.0\n",
            "Average batch loss: 0.014243484198232181\n",
            "Step: 32, Action: 1, Done: False, Portfolio Value: 9999.060576629692, Reward: 0.0\n",
            "Average batch loss: 0.00553807450523891\n",
            "Step: 33, Action: 2, Done: False, Portfolio Value: 9999.060576629692, Reward: 0.0\n",
            "Average batch loss: 0.001790981434112382\n",
            "Step: 34, Action: 1, Done: False, Portfolio Value: 9999.060576629692, Reward: 0.0\n",
            "Average batch loss: 0.0014501822608012915\n",
            "Step: 35, Action: 2, Done: False, Portfolio Value: 9999.060576629692, Reward: 0.0\n",
            "Average batch loss: 0.0035967206131317653\n",
            "Step: 36, Action: 1, Done: False, Portfolio Value: 9999.060576629692, Reward: 0.0\n",
            "Average batch loss: 0.004361415359966259\n",
            "Step: 37, Action: 2, Done: False, Portfolio Value: 9999.060576629692, Reward: 0.0\n",
            "Average batch loss: 6.882526703178883\n",
            "Step: 38, Action: 2, Done: False, Portfolio Value: 9999.060576629692, Reward: 0.0\n",
            "Average batch loss: 0.6229944248916581\n",
            "Step: 39, Action: 1, Done: False, Portfolio Value: 9999.060576629692, Reward: 0.0\n",
            "Average batch loss: 0.2931176817510277\n",
            "Step: 40, Action: 2, Done: False, Portfolio Value: 9999.060576629692, Reward: 0.0\n",
            "Average batch loss: 0.11435967034776695\n",
            "Step: 41, Action: 0, Done: False, Portfolio Value: 9999.060576629692, Reward: 0.0\n",
            "Average batch loss: 0.014790103273298882\n",
            "Step: 42, Action: 2, Done: False, Portfolio Value: 9998.640317696198, Reward: 0.0\n",
            "Average batch loss: 0.007145080689355154\n",
            "Step: 43, Action: 2, Done: False, Portfolio Value: 9998.676670404053, Reward: 0.0\n",
            "Average batch loss: 0.0024936259906098712\n",
            "Step: 44, Action: 1, Done: False, Portfolio Value: 9998.699767719423, Reward: 0.0\n",
            "Average batch loss: 0.004040406111926131\n",
            "Step: 45, Action: 1, Done: False, Portfolio Value: 9998.699767719423, Reward: 0.0\n",
            "Average batch loss: 0.0025389843846568283\n",
            "Step: 46, Action: 0, Done: False, Portfolio Value: 9998.699767719423, Reward: 0.0\n",
            "Average batch loss: 0.00277766054691142\n",
            "Step: 47, Action: 0, Done: False, Portfolio Value: 9998.70996040408, Reward: 0.0\n",
            "Average batch loss: 7.276545182103291\n",
            "Step: 48, Action: 2, Done: False, Portfolio Value: 9998.92961245223, Reward: 0.0\n",
            "Average batch loss: 2.6985717008356005\n",
            "Step: 49, Action: 0, Done: False, Portfolio Value: 9998.669683047556, Reward: -1.8189894035458565e-12\n",
            "Average batch loss: 0.35852787806652486\n",
            "Step: 50, Action: 1, Done: False, Portfolio Value: 9999.554737123213, Reward: 0.0\n",
            "Average batch loss: 0.2827057659160346\n",
            "Step: 51, Action: 1, Done: False, Portfolio Value: 9999.465892700291, Reward: 0.0\n",
            "Average batch loss: 0.04616929788608104\n",
            "Step: 52, Action: 2, Done: False, Portfolio Value: 9999.244219858421, Reward: 0.0\n",
            "Average batch loss: 0.013914223668820114\n",
            "Step: 53, Action: 1, Done: False, Portfolio Value: 9999.24844827853, Reward: 0.0\n",
            "Average batch loss: 0.008061531260864285\n",
            "Step: 54, Action: 1, Done: False, Portfolio Value: 9999.24844827853, Reward: 0.0\n",
            "Average batch loss: 0.005369684207835235\n",
            "Step: 55, Action: 0, Done: False, Portfolio Value: 9999.24844827853, Reward: 0.0\n",
            "Average batch loss: 0.0022542757371866173\n",
            "Step: 56, Action: 1, Done: False, Portfolio Value: 9999.210051965281, Reward: 0.0\n",
            "Average batch loss: 0.004009359929398215\n",
            "Step: 57, Action: 0, Done: False, Portfolio Value: 9999.210051965281, Reward: 0.0\n",
            "Average batch loss: 5.8920184721646365\n",
            "Step: 58, Action: 2, Done: False, Portfolio Value: 9998.942010249832, Reward: 0.0\n",
            "Average batch loss: 1.2944188720430247\n",
            "Step: 59, Action: 2, Done: False, Portfolio Value: 9999.386205275072, Reward: 0.0\n",
            "Average batch loss: 0.13592782962950878\n",
            "Step: 60, Action: 0, Done: False, Portfolio Value: 9999.451135163612, Reward: 1.8189894035458565e-12\n",
            "Average batch loss: 0.015961065405917907\n",
            "Step: 61, Action: 2, Done: False, Portfolio Value: 9998.462689914188, Reward: 0.0\n",
            "Average batch loss: 0.008089220258625573\n",
            "Step: 62, Action: 0, Done: False, Portfolio Value: 9998.523265558551, Reward: -1.8189894035458565e-12\n",
            "Average batch loss: 0.0032033297499438618\n",
            "Step: 63, Action: 0, Done: False, Portfolio Value: 9999.824561166632, Reward: 0.0\n",
            "Average batch loss: 0.007091281399880245\n",
            "Step: 64, Action: 0, Done: False, Portfolio Value: 9998.237769558058, Reward: 1.8189894035458565e-12\n",
            "Average batch loss: 0.009400899734828272\n",
            "Step: 65, Action: 1, Done: False, Portfolio Value: 9997.937704528056, Reward: 0.0\n",
            "Average batch loss: 0.01888907817677321\n",
            "Step: 66, Action: 1, Done: False, Portfolio Value: 9999.21930167572, Reward: 0.0\n",
            "Average batch loss: 0.02040702690677154\n",
            "Step: 67, Action: 2, Done: False, Portfolio Value: 9998.274050765353, Reward: 0.0\n",
            "Average batch loss: 5.046766331244726\n",
            "Step: 68, Action: 2, Done: False, Portfolio Value: 9998.206718222878, Reward: 0.0\n",
            "Average batch loss: 1.1976695181801915\n",
            "Step: 69, Action: 2, Done: False, Portfolio Value: 9998.29287159913, Reward: 0.0\n",
            "Average batch loss: 0.534694098944783\n",
            "Step: 70, Action: 0, Done: False, Portfolio Value: 9998.22171631634, Reward: -1.8189894035458565e-12\n",
            "Average batch loss: 0.10613375304819783\n",
            "Step: 71, Action: 1, Done: False, Portfolio Value: 9998.214419315836, Reward: 0.0\n",
            "Average batch loss: 0.03659460592950836\n",
            "Step: 72, Action: 0, Done: False, Portfolio Value: 10000.967046648548, Reward: 0.0\n",
            "Average batch loss: 0.019911891024094075\n",
            "Step: 73, Action: 2, Done: False, Portfolio Value: 9997.335057596561, Reward: 0.0\n",
            "Average batch loss: 0.011165285912284162\n",
            "Step: 74, Action: 2, Done: False, Portfolio Value: 9997.25466765541, Reward: 0.0\n",
            "Average batch loss: 0.01752587620057966\n",
            "Step: 75, Action: 2, Done: False, Portfolio Value: 9997.33401857381, Reward: 0.0\n",
            "Average batch loss: 0.018526209472241817\n",
            "Step: 76, Action: 1, Done: False, Portfolio Value: 9997.436153643996, Reward: 0.0\n",
            "Average batch loss: 0.018518141647746234\n",
            "Step: 77, Action: 0, Done: False, Portfolio Value: 9997.601487015567, Reward: 0.0\n",
            "Average batch loss: 4.984097314843893\n",
            "Step: 78, Action: 2, Done: False, Portfolio Value: 9999.251549327779, Reward: 0.0\n",
            "Average batch loss: 1.0244348396663554\n",
            "Step: 79, Action: 1, Done: False, Portfolio Value: 9998.252148413952, Reward: 0.0\n",
            "Average batch loss: 0.16137530162086478\n",
            "Step: 80, Action: 1, Done: False, Portfolio Value: 9999.310717128405, Reward: 0.0\n",
            "Average batch loss: 0.02138374842547819\n",
            "Step: 81, Action: 0, Done: False, Portfolio Value: 9998.312444254278, Reward: 0.0\n",
            "Average batch loss: 0.011393328900339839\n",
            "Step: 82, Action: 2, Done: False, Portfolio Value: 9998.03219932018, Reward: 0.0\n",
            "Average batch loss: 0.009912301774723797\n",
            "Step: 83, Action: 0, Done: False, Portfolio Value: 9998.245774613768, Reward: 0.0\n",
            "Average batch loss: 0.013323587523998981\n",
            "Step: 84, Action: 0, Done: False, Portfolio Value: 9997.891308701353, Reward: 0.0\n",
            "Average batch loss: 0.014992547050496796\n",
            "Step: 85, Action: 1, Done: False, Portfolio Value: 9998.031570579435, Reward: 0.0\n",
            "Average batch loss: 0.012341372689490981\n",
            "Step: 86, Action: 2, Done: False, Portfolio Value: 9997.926186400638, Reward: 0.0\n",
            "Average batch loss: 0.012639226578965435\n",
            "Step: 87, Action: 2, Done: False, Portfolio Value: 9997.97280225547, Reward: 0.0\n",
            "Average batch loss: 4.835694119411841\n",
            "Step: 88, Action: 0, Done: False, Portfolio Value: 9998.711909730559, Reward: 1.8189894035458565e-12\n",
            "Average batch loss: 0.8577089048922062\n",
            "Step: 89, Action: 1, Done: False, Portfolio Value: 9999.24723828014, Reward: 0.0\n",
            "Average batch loss: 0.18474782242265064\n",
            "Step: 90, Action: 1, Done: False, Portfolio Value: 9998.375476473113, Reward: -1.8189894035458565e-12\n",
            "Average batch loss: 0.059281783779624675\n",
            "Step: 91, Action: 0, Done: False, Portfolio Value: 9998.31372807649, Reward: 0.0\n",
            "Average batch loss: 0.037748146610283584\n",
            "Step: 92, Action: 0, Done: False, Portfolio Value: 9998.749744311513, Reward: 0.0\n",
            "Average batch loss: 0.014896728349413024\n",
            "Step: 93, Action: 2, Done: False, Portfolio Value: 10000.966397088925, Reward: 0.0\n",
            "Average batch loss: 0.02070903339608776\n",
            "Step: 94, Action: 2, Done: False, Portfolio Value: 9997.908046455532, Reward: 0.0\n",
            "Average batch loss: 0.014366721981787123\n",
            "Step: 95, Action: 0, Done: False, Portfolio Value: 9997.92666258553, Reward: 1.8189894035458565e-12\n",
            "Average batch loss: 0.018112124641447735\n",
            "Step: 96, Action: 2, Done: False, Portfolio Value: 9998.147602673414, Reward: 0.0\n",
            "Average batch loss: 0.032686025636394334\n",
            "Step: 97, Action: 2, Done: False, Portfolio Value: 9998.38067503542, Reward: 0.0\n",
            "Average batch loss: 4.180390565044945\n",
            "Step: 98, Action: 1, Done: False, Portfolio Value: 9999.13790298918, Reward: 0.0\n",
            "Average batch loss: 2.19402026780881\n",
            "Step: 99, Action: 2, Done: False, Portfolio Value: 10001.232457245454, Reward: 0.0\n",
            "Average batch loss: 0.38904420906328596\n",
            "Step: 100, Action: 0, Done: False, Portfolio Value: 10000.16538804631, Reward: -1.8189894035458565e-12\n",
            "Average batch loss: 0.18324904576456902\n",
            "Step: 101, Action: 2, Done: False, Portfolio Value: 9997.724190650422, Reward: 0.0\n",
            "Average batch loss: 0.04679262424664188\n",
            "Step: 102, Action: 2, Done: False, Portfolio Value: 9997.714014965297, Reward: 0.0\n",
            "Average batch loss: 0.04374469058484465\n",
            "Step: 103, Action: 2, Done: False, Portfolio Value: 9998.528004035348, Reward: 0.0\n",
            "Average batch loss: 0.041127800150967175\n",
            "Step: 104, Action: 1, Done: False, Portfolio Value: 9997.791707798042, Reward: 0.0\n",
            "Average batch loss: 0.05777925047732424\n",
            "Step: 105, Action: 0, Done: False, Portfolio Value: 9997.7806645715, Reward: 0.0\n",
            "Average batch loss: 0.02009488525368397\n",
            "Step: 106, Action: 1, Done: False, Portfolio Value: 9997.992421582388, Reward: 0.0\n",
            "Average batch loss: 0.018573323869205183\n",
            "Step: 107, Action: 2, Done: False, Portfolio Value: 9998.341507517009, Reward: 0.0\n",
            "Average batch loss: 3.713437774742488\n",
            "Step: 108, Action: 0, Done: False, Portfolio Value: 9998.513461098952, Reward: -1.8189894035458565e-12\n",
            "Average batch loss: 3.170157832559198\n",
            "Step: 109, Action: 1, Done: False, Portfolio Value: 9999.499717387962, Reward: 0.0\n",
            "Average batch loss: 0.6678032813651953\n",
            "Step: 110, Action: 1, Done: False, Portfolio Value: 9998.175108310923, Reward: 0.0\n",
            "Average batch loss: 0.424337377800839\n",
            "Step: 111, Action: 2, Done: False, Portfolio Value: 9998.026453837863, Reward: 0.0\n",
            "Average batch loss: 0.1606728776532691\n",
            "Step: 112, Action: 2, Done: False, Portfolio Value: 9998.348067883768, Reward: 0.0\n",
            "Average batch loss: 0.0891640530735458\n",
            "Step: 113, Action: 1, Done: False, Portfolio Value: 9998.290009610635, Reward: 0.0\n",
            "Average batch loss: 0.042102137333131395\n",
            "Step: 114, Action: 0, Done: False, Portfolio Value: 9998.212903682837, Reward: -1.8189894035458565e-12\n",
            "Average batch loss: 0.03809949351625619\n",
            "Step: 115, Action: 0, Done: False, Portfolio Value: 9998.984124642502, Reward: 0.0\n",
            "Average batch loss: 0.05004166929325038\n",
            "Step: 116, Action: 0, Done: False, Portfolio Value: 9997.710325470824, Reward: 0.0\n",
            "Average batch loss: 0.031004397964995434\n",
            "Step: 117, Action: 2, Done: False, Portfolio Value: 9998.197934778538, Reward: 0.0\n",
            "Average batch loss: 4.295609039720148\n",
            "Step: 118, Action: 2, Done: False, Portfolio Value: 10000.250271232238, Reward: 0.0\n",
            "Average batch loss: 1.6208682892224715\n",
            "Step: 119, Action: 1, Done: False, Portfolio Value: 9997.702642920483, Reward: 0.0\n",
            "Average batch loss: 0.9109210669994354\n",
            "Step: 120, Action: 0, Done: False, Portfolio Value: 9997.866466673884, Reward: 1.8189894035458565e-12\n",
            "Average batch loss: 0.2665658891783096\n",
            "Step: 121, Action: 1, Done: False, Portfolio Value: 9998.020993253962, Reward: 0.0\n",
            "Average batch loss: 0.06511378709319615\n",
            "Step: 122, Action: 0, Done: False, Portfolio Value: 10000.528319204373, Reward: -1.8189894035458565e-12\n",
            "Average batch loss: 0.11759841767161561\n",
            "Step: 123, Action: 0, Done: False, Portfolio Value: 10000.172715900284, Reward: 0.0\n",
            "Average batch loss: 0.09603976768266875\n",
            "Step: 124, Action: 0, Done: False, Portfolio Value: 9996.780157687468, Reward: 1.8189894035458565e-12\n",
            "Average batch loss: 0.04487524317300995\n",
            "Step: 125, Action: 1, Done: False, Portfolio Value: 9997.326687054363, Reward: 0.0\n",
            "Average batch loss: 0.04299990266190434\n",
            "Step: 126, Action: 2, Done: False, Portfolio Value: 9996.74556811749, Reward: 0.0\n",
            "Average batch loss: 0.09003225167907658\n",
            "Step: 127, Action: 1, Done: False, Portfolio Value: 9998.022913087567, Reward: 0.0\n",
            "Average batch loss: 5.976371515542269\n",
            "Step: 128, Action: 1, Done: False, Portfolio Value: 10000.167910039581, Reward: 0.0\n",
            "Average batch loss: 2.6927555958682206\n",
            "Step: 129, Action: 0, Done: False, Portfolio Value: 9997.630798511796, Reward: 0.0\n",
            "Average batch loss: 0.6870805342477979\n",
            "Step: 130, Action: 2, Done: False, Portfolio Value: 9997.438518735604, Reward: 0.0\n",
            "Average batch loss: 0.30624477570199815\n",
            "Step: 131, Action: 2, Done: False, Portfolio Value: 9997.468222224787, Reward: 0.0\n",
            "Average batch loss: 0.25611442197805445\n",
            "Step: 132, Action: 0, Done: False, Portfolio Value: 9998.353813578267, Reward: 0.0\n",
            "Average batch loss: 0.12879264996445272\n",
            "Step: 133, Action: 0, Done: False, Portfolio Value: 9998.983323868842, Reward: 1.8189894035458565e-12\n",
            "Average batch loss: 0.062049946216575336\n",
            "Step: 134, Action: 0, Done: False, Portfolio Value: 9997.918561466451, Reward: 0.0\n",
            "Average batch loss: 0.08057068946072832\n",
            "Step: 135, Action: 2, Done: False, Portfolio Value: 9998.076576873069, Reward: 0.0\n",
            "Average batch loss: 0.03438732531503774\n",
            "Step: 136, Action: 2, Done: False, Portfolio Value: 9996.889355385569, Reward: 0.0\n",
            "Average batch loss: 0.05417381892038975\n",
            "Step: 137, Action: 1, Done: False, Portfolio Value: 9997.620427095762, Reward: 0.0\n",
            "Average batch loss: 4.98768821451813\n",
            "Step: 138, Action: 2, Done: False, Portfolio Value: 9997.012567481792, Reward: 0.0\n",
            "Average batch loss: 1.8086321356458939\n",
            "Step: 139, Action: 0, Done: False, Portfolio Value: 9997.50964016515, Reward: 0.0\n",
            "Average batch loss: 0.5939548730384558\n",
            "Step: 140, Action: 0, Done: False, Portfolio Value: 10005.260167060775, Reward: 0.0\n",
            "Average batch loss: 0.8949124058271991\n",
            "Step: 141, Action: 2, Done: False, Portfolio Value: 10002.573625395888, Reward: 0.0\n",
            "Average batch loss: 0.108001529170906\n",
            "Step: 142, Action: 1, Done: False, Portfolio Value: 9997.721299888482, Reward: 0.0\n",
            "Average batch loss: 0.0640936133422656\n",
            "Step: 143, Action: 2, Done: False, Portfolio Value: 9998.45449629329, Reward: 0.0\n",
            "Average batch loss: 0.11785704366866412\n",
            "Step: 144, Action: 2, Done: False, Portfolio Value: 9996.82490674768, Reward: 0.0\n",
            "Average batch loss: 0.298262743216128\n",
            "Step: 145, Action: 2, Done: False, Portfolio Value: 9998.190236082544, Reward: 0.0\n",
            "Average batch loss: 0.140329183992435\n",
            "Step: 146, Action: 1, Done: False, Portfolio Value: 9998.07260814479, Reward: 0.0\n",
            "Average batch loss: 0.25207535373086376\n",
            "Step: 147, Action: 2, Done: False, Portfolio Value: 9997.248619526634, Reward: 0.0\n",
            "Average batch loss: 5.365165413124487\n",
            "Step: 148, Action: 2, Done: False, Portfolio Value: 9999.029970566977, Reward: 0.0\n",
            "Average batch loss: 0.7117667848360725\n",
            "Step: 149, Action: 0, Done: False, Portfolio Value: 10001.142825692215, Reward: 0.0\n",
            "Average batch loss: 0.5993098876497243\n",
            "Step: 150, Action: 2, Done: False, Portfolio Value: 9997.20825789378, Reward: 0.0\n",
            "Average batch loss: 0.25698911215295084\n",
            "Step: 151, Action: 0, Done: False, Portfolio Value: 9995.763438772421, Reward: 0.0\n",
            "Average batch loss: 0.2003716161743796\n",
            "Step: 152, Action: 1, Done: False, Portfolio Value: 9999.078965012592, Reward: 0.0\n",
            "Average batch loss: 0.12698188556009882\n",
            "Step: 153, Action: 2, Done: False, Portfolio Value: 9999.446471316813, Reward: 0.0\n",
            "Average batch loss: 0.14920992302359082\n",
            "Step: 154, Action: 1, Done: False, Portfolio Value: 9997.661486346056, Reward: 0.0\n",
            "Average batch loss: 0.14130432488430777\n",
            "Step: 155, Action: 2, Done: False, Portfolio Value: 10002.602181174118, Reward: 0.0\n",
            "Average batch loss: 0.10062495226338797\n",
            "Step: 156, Action: 2, Done: False, Portfolio Value: 9997.518194097207, Reward: 0.0\n",
            "Average batch loss: 0.06823423872992862\n",
            "Step: 157, Action: 2, Done: False, Portfolio Value: 9996.329574334972, Reward: 0.0\n",
            "Average batch loss: 4.9143402157351375\n",
            "Step: 158, Action: 1, Done: False, Portfolio Value: 9996.349861375988, Reward: 0.0\n",
            "Average batch loss: 1.435662904754281\n",
            "Step: 159, Action: 0, Done: False, Portfolio Value: 9996.426660120613, Reward: 0.0\n",
            "Average batch loss: 1.1477852817624807\n",
            "Step: 160, Action: 0, Done: False, Portfolio Value: 9996.314211238277, Reward: 0.0\n",
            "Average batch loss: 0.5348732928299285\n",
            "Step: 161, Action: 1, Done: False, Portfolio Value: 9998.135181155407, Reward: 0.0\n",
            "Average batch loss: 0.35622927971417084\n",
            "Step: 162, Action: 0, Done: False, Portfolio Value: 9996.550265755612, Reward: -1.8189894035458565e-12\n",
            "Average batch loss: 0.18244189666256716\n",
            "Step: 163, Action: 0, Done: False, Portfolio Value: 9999.259031222791, Reward: 0.0\n",
            "Average batch loss: 0.16005452424451505\n",
            "Step: 164, Action: 2, Done: False, Portfolio Value: 10000.9735285393, Reward: 0.0\n",
            "Average batch loss: 0.1385735030453361\n",
            "Step: 165, Action: 2, Done: False, Portfolio Value: 10000.590651193568, Reward: 0.0\n",
            "Average batch loss: 0.2557923140315097\n",
            "Step: 166, Action: 0, Done: False, Portfolio Value: 9997.308326472767, Reward: 0.0\n",
            "Average batch loss: 0.17414838654804043\n",
            "Step: 167, Action: 1, Done: False, Portfolio Value: 9996.157695447939, Reward: 0.0\n",
            "Average batch loss: 4.302451135008596\n",
            "Step: 168, Action: 2, Done: False, Portfolio Value: 10004.57765357844, Reward: 0.0\n",
            "Average batch loss: 1.2115072899214283\n",
            "Step: 169, Action: 0, Done: False, Portfolio Value: 9998.991899936975, Reward: -1.8189894035458565e-12\n",
            "Average batch loss: 0.3492304621322546\n",
            "Step: 170, Action: 1, Done: False, Portfolio Value: 9995.81443876478, Reward: 0.0\n",
            "Average batch loss: 0.2600991478539072\n",
            "Step: 171, Action: 1, Done: False, Portfolio Value: 9997.3050306193, Reward: 1.8189894035458565e-12\n",
            "Average batch loss: 0.13024736654188018\n",
            "Step: 172, Action: 0, Done: False, Portfolio Value: 9998.170800935746, Reward: 0.0\n",
            "Average batch loss: 0.1573066382916295\n",
            "Step: 173, Action: 2, Done: False, Portfolio Value: 9995.780604096337, Reward: 0.0\n",
            "Average batch loss: 0.1080030540961161\n",
            "Step: 174, Action: 0, Done: False, Portfolio Value: 9996.222200043056, Reward: 0.0\n",
            "Average batch loss: 0.08484377870991011\n",
            "Step: 175, Action: 1, Done: False, Portfolio Value: 9995.87078123852, Reward: 1.8189894035458565e-12\n",
            "Average batch loss: 0.09150406171102077\n",
            "Step: 176, Action: 2, Done: False, Portfolio Value: 9996.274346269098, Reward: 0.0\n",
            "Average batch loss: 0.0757233221605702\n",
            "Step: 177, Action: 2, Done: False, Portfolio Value: 9999.006549414196, Reward: 0.0\n",
            "Average batch loss: 5.6066317949444056\n",
            "Step: 178, Action: 0, Done: False, Portfolio Value: 9996.575720693842, Reward: 0.0\n",
            "Average batch loss: 12.206821198575199\n",
            "Step: 179, Action: 0, Done: False, Portfolio Value: 10000.690981390695, Reward: 0.0\n",
            "Average batch loss: 2.8260775885319163\n",
            "Step: 180, Action: 0, Done: False, Portfolio Value: 9998.947854775886, Reward: 0.0\n",
            "Average batch loss: 0.5495684004890791\n",
            "Step: 181, Action: 2, Done: False, Portfolio Value: 9994.8065531649, Reward: 0.0\n",
            "Average batch loss: 0.12526136040105484\n",
            "Step: 182, Action: 1, Done: False, Portfolio Value: 9994.869504313498, Reward: 1.8189894035458565e-12\n",
            "Average batch loss: 0.07444486231543124\n",
            "Step: 183, Action: 1, Done: False, Portfolio Value: 9998.62195721659, Reward: 0.0\n",
            "Average batch loss: 0.11262810746848118\n",
            "Step: 184, Action: 0, Done: False, Portfolio Value: 9995.118992028221, Reward: 0.0\n",
            "Average batch loss: 0.08107500720416283\n",
            "Step: 185, Action: 0, Done: False, Portfolio Value: 9995.551681858147, Reward: 0.0\n",
            "Average batch loss: 0.030013970797881484\n",
            "Step: 186, Action: 2, Done: False, Portfolio Value: 9996.78843462257, Reward: 0.0\n",
            "Average batch loss: 0.08142336735909339\n",
            "Step: 187, Action: 2, Done: False, Portfolio Value: 9997.946916395913, Reward: 0.0\n",
            "Average batch loss: 5.926451765932143\n",
            "Step: 188, Action: 2, Done: False, Portfolio Value: 10001.936451435724, Reward: 0.0\n",
            "Average batch loss: 3.430009886928019\n",
            "Step: 189, Action: 0, Done: False, Portfolio Value: 9995.82661617063, Reward: 0.0\n",
            "Average batch loss: 0.8602755685569718\n",
            "Step: 190, Action: 2, Done: False, Portfolio Value: 9995.533724709954, Reward: 0.0\n",
            "Average batch loss: 0.33405897530610673\n",
            "Step: 191, Action: 2, Done: False, Portfolio Value: 9999.387947983696, Reward: 0.0\n",
            "Average batch loss: 0.09567289166898263\n",
            "Step: 192, Action: 2, Done: False, Portfolio Value: 9995.310135846492, Reward: 0.0\n",
            "Average batch loss: 0.1690208234504098\n",
            "Step: 193, Action: 0, Done: False, Portfolio Value: 9997.105815049861, Reward: 0.0\n",
            "Average batch loss: 0.05307198394666557\n",
            "Step: 194, Action: 0, Done: False, Portfolio Value: 9995.284557175273, Reward: 1.8189894035458565e-12\n",
            "Average batch loss: 0.27713985935156416\n",
            "Step: 195, Action: 1, Done: False, Portfolio Value: 9995.274427494833, Reward: 0.0\n",
            "Average batch loss: 0.47183842887170613\n",
            "Step: 196, Action: 2, Done: False, Portfolio Value: 10000.292796005298, Reward: 0.0\n",
            "Average batch loss: 0.28153817071870435\n",
            "Step: 197, Action: 2, Done: False, Portfolio Value: 9997.004856731604, Reward: 0.0\n",
            "Average batch loss: 8.280714256688952\n",
            "Step: 198, Action: 1, Done: False, Portfolio Value: 9994.90502071474, Reward: 0.0\n",
            "Average batch loss: 6.474171656027465\n",
            "Step: 199, Action: 2, Done: False, Portfolio Value: 9995.27329796655, Reward: 0.0\n",
            "Average batch loss: 1.9515219337772578\n",
            "Step: 200, Action: 0, Done: False, Portfolio Value: 9995.36590575951, Reward: 0.0\n",
            "Average batch loss: 1.3983747567981482\n",
            "Step: 201, Action: 2, Done: False, Portfolio Value: 9995.679027745435, Reward: 0.0\n",
            "Average batch loss: 0.8886147017365147\n",
            "Step: 202, Action: 2, Done: False, Portfolio Value: 9997.709446573344, Reward: 0.0\n",
            "Average batch loss: 1.085216767154634\n",
            "Step: 203, Action: 0, Done: False, Portfolio Value: 9998.415899643023, Reward: 0.0\n",
            "Average batch loss: 0.5792496139956711\n",
            "Step: 204, Action: 1, Done: False, Portfolio Value: 9997.13354021314, Reward: 0.0\n",
            "Average batch loss: 0.7200288817693945\n",
            "Step: 205, Action: 1, Done: False, Portfolio Value: 9997.23978422603, Reward: 0.0\n",
            "Average batch loss: 0.8259128610006883\n",
            "Step: 206, Action: 2, Done: False, Portfolio Value: 9995.001995499382, Reward: 0.0\n",
            "Average batch loss: 0.753724351602898\n",
            "Step: 207, Action: 2, Done: False, Portfolio Value: 9996.911414752316, Reward: 0.0\n",
            "Average batch loss: 6.379545241827145\n",
            "Step: 208, Action: 2, Done: False, Portfolio Value: 9994.92192552917, Reward: 0.0\n",
            "Average batch loss: 2.2269122678553686\n",
            "Step: 209, Action: 1, Done: False, Portfolio Value: 9998.847803595516, Reward: -1.8189894035458565e-12\n",
            "Average batch loss: 1.256137546384707\n",
            "Step: 210, Action: 0, Done: False, Portfolio Value: 9996.109134900234, Reward: -1.8189894035458565e-12\n",
            "Average batch loss: 2.2346376916393638\n",
            "Step: 211, Action: 2, Done: False, Portfolio Value: 9995.372888442183, Reward: 0.0\n",
            "Average batch loss: 0.7581552991905482\n",
            "Step: 212, Action: 1, Done: False, Portfolio Value: 9999.571312314853, Reward: 0.0\n",
            "Average batch loss: 0.3345393047984544\n",
            "Step: 213, Action: 0, Done: False, Portfolio Value: 9997.355275752601, Reward: 0.0\n",
            "Average batch loss: 0.16905527074914062\n",
            "Step: 214, Action: 0, Done: False, Portfolio Value: 9995.296403899152, Reward: 0.0\n",
            "Average batch loss: 0.05227688682862208\n",
            "Step: 215, Action: 2, Done: False, Portfolio Value: 9995.37258325027, Reward: 0.0\n",
            "Average batch loss: 0.07404788583880872\n",
            "Step: 216, Action: 0, Done: False, Portfolio Value: 9995.311060316068, Reward: 1.8189894035458565e-12\n",
            "Average batch loss: 0.15865217351529282\n",
            "Step: 217, Action: 2, Done: False, Portfolio Value: 9996.318006173105, Reward: 0.0\n",
            "Average batch loss: 6.8450857708230615\n",
            "Step: 218, Action: 2, Done: False, Portfolio Value: 9998.68979766951, Reward: 0.0\n",
            "Average batch loss: 2.380869261920452\n",
            "Step: 219, Action: 2, Done: False, Portfolio Value: 9998.578903406238, Reward: 0.0\n",
            "Average batch loss: 0.39608784615120385\n",
            "Step: 220, Action: 2, Done: False, Portfolio Value: 9996.791611067503, Reward: 0.0\n",
            "Average batch loss: 0.5276584917955915\n",
            "Step: 221, Action: 2, Done: False, Portfolio Value: 9997.633336797064, Reward: 0.0\n",
            "Average batch loss: 0.22742611399735324\n",
            "Step: 222, Action: 2, Done: False, Portfolio Value: 9996.541205075466, Reward: 0.0\n",
            "Average batch loss: 0.10803130848307774\n",
            "Step: 223, Action: 2, Done: False, Portfolio Value: 10003.379712093725, Reward: 0.0\n",
            "Average batch loss: 0.23437418311368674\n",
            "Step: 224, Action: 1, Done: False, Portfolio Value: 10002.810683491381, Reward: 0.0\n",
            "Average batch loss: 0.38826611218973994\n",
            "Step: 225, Action: 0, Done: False, Portfolio Value: 10000.646956483139, Reward: 0.0\n",
            "Average batch loss: 0.5190020124036892\n",
            "Step: 226, Action: 0, Done: False, Portfolio Value: 10010.815830534744, Reward: 0.0\n",
            "Average batch loss: 0.3606998029863462\n",
            "Step: 227, Action: 2, Done: False, Portfolio Value: 9994.743152056159, Reward: 0.0\n",
            "Average batch loss: 5.5931654828600585\n",
            "Step: 228, Action: 0, Done: False, Portfolio Value: 9995.247604645521, Reward: 0.0\n",
            "Average batch loss: 0.9758893880061805\n",
            "Step: 229, Action: 1, Done: False, Portfolio Value: 10004.549808598433, Reward: -1.8189894035458565e-12\n",
            "Average batch loss: 6.166650811775298\n",
            "Step: 230, Action: 2, Done: False, Portfolio Value: 10004.961678876243, Reward: 0.0\n",
            "Average batch loss: 5.23418240994215\n",
            "Step: 231, Action: 0, Done: False, Portfolio Value: 9994.93166135687, Reward: 0.0\n",
            "Average batch loss: 8.005644657028217\n",
            "Step: 232, Action: 2, Done: False, Portfolio Value: 10001.73633537114, Reward: 0.0\n",
            "Average batch loss: 4.673859076574445\n",
            "Step: 233, Action: 0, Done: False, Portfolio Value: 9996.446085269685, Reward: 0.0\n",
            "Average batch loss: 1.1157553022203501\n",
            "Step: 234, Action: 2, Done: False, Portfolio Value: 9995.975761274922, Reward: 0.0\n",
            "Average batch loss: 0.8725156509317458\n",
            "Step: 235, Action: 0, Done: False, Portfolio Value: 9994.979522420208, Reward: 0.0\n",
            "Average batch loss: 3.8826589883829\n",
            "Step: 236, Action: 0, Done: False, Portfolio Value: 10008.350627650068, Reward: 0.0\n",
            "Average batch loss: 2.310232263058424\n",
            "Step: 237, Action: 1, Done: False, Portfolio Value: 9997.800351377471, Reward: 0.0\n",
            "Average batch loss: 14.477017219178379\n",
            "Step: 238, Action: 1, Done: False, Portfolio Value: 9994.527371150321, Reward: 1.8189894035458565e-12\n",
            "Average batch loss: 9.320540763437748\n",
            "Step: 239, Action: 2, Done: False, Portfolio Value: 9994.914382575527, Reward: 0.0\n",
            "Average batch loss: 4.150849780147837\n",
            "Step: 240, Action: 2, Done: False, Portfolio Value: 9994.44502691448, Reward: 0.0\n",
            "Average batch loss: 1.9182782453135587\n",
            "Step: 241, Action: 1, Done: False, Portfolio Value: 9995.926020791674, Reward: 1.8189894035458565e-12\n",
            "Average batch loss: 1.1027356581471395\n",
            "Step: 242, Action: 2, Done: False, Portfolio Value: 9996.896482625632, Reward: 0.0\n",
            "Average batch loss: 1.7249352621438447\n",
            "Step: 243, Action: 2, Done: False, Portfolio Value: 9996.63342923334, Reward: 0.0\n",
            "Average batch loss: 2.6616985769942403\n",
            "Step: 244, Action: 0, Done: False, Portfolio Value: 10002.102265086662, Reward: 0.0\n",
            "Average batch loss: 1.720859113149345\n",
            "Step: 245, Action: 2, Done: False, Portfolio Value: 10012.039380526521, Reward: 0.0\n",
            "Average batch loss: 3.3373543523121043\n",
            "Step: 246, Action: 2, Done: False, Portfolio Value: 10000.01189591215, Reward: 0.0\n",
            "Average batch loss: 3.6801580124883913\n",
            "Step: 247, Action: 2, Done: False, Portfolio Value: 9996.587593824039, Reward: 0.0\n",
            "Average batch loss: 59.617585729807615\n",
            "Step: 248, Action: 0, Done: False, Portfolio Value: 10000.155151000547, Reward: 0.0\n",
            "Average batch loss: 151.04722632467747\n",
            "Step: 249, Action: 1, Done: False, Portfolio Value: 10010.625828149841, Reward: -1.8189894035458565e-12\n",
            "Average batch loss: 66.05481460737064\n",
            "Step: 250, Action: 0, Done: False, Portfolio Value: 9998.16659838678, Reward: 0.0\n",
            "Average batch loss: 247.86543836537749\n",
            "Step: 251, Action: 2, Done: False, Portfolio Value: 9997.061196198038, Reward: 0.0\n",
            "Average batch loss: 275.57941015064716\n",
            "Step: 252, Action: 2, Done: False, Portfolio Value: 9999.914302104513, Reward: 0.0\n",
            "Average batch loss: 183.00564116239548\n",
            "Step: 253, Action: 1, Done: False, Portfolio Value: 9998.857474754745, Reward: 0.0\n",
            "Average batch loss: 212.07631132006645\n",
            "Step: 254, Action: 0, Done: False, Portfolio Value: 9995.888425526802, Reward: 0.0\n",
            "Average batch loss: 156.89169016480446\n",
            "Step: 255, Action: 0, Done: False, Portfolio Value: 9994.740466968791, Reward: 0.0\n",
            "Average batch loss: 178.3477644994855\n",
            "Step: 256, Action: 0, Done: False, Portfolio Value: 9994.621933302284, Reward: 1.8189894035458565e-12\n",
            "Average batch loss: 41.544604927592445\n",
            "Step: 257, Action: 1, Done: False, Portfolio Value: 10007.405474246578, Reward: 0.0\n",
            "Average batch loss: 25.187712164595723\n",
            "Step: 258, Action: 0, Done: False, Portfolio Value: 9999.411629757513, Reward: 0.0\n",
            "Average batch loss: 29.23106292221928\n",
            "Step: 259, Action: 0, Done: False, Portfolio Value: 9994.802668394339, Reward: 0.0\n",
            "Average batch loss: 23.807430505752563\n",
            "Step: 260, Action: 0, Done: False, Portfolio Value: 9994.933145694116, Reward: 0.0\n",
            "Average batch loss: 115.13240101281554\n",
            "Step: 261, Action: 2, Done: False, Portfolio Value: 9995.53986641226, Reward: 0.0\n",
            "Average batch loss: 106.50873791053891\n",
            "Step: 262, Action: 2, Done: False, Portfolio Value: 9996.464503959074, Reward: 0.0\n",
            "Average batch loss: 40.60204486781731\n",
            "Step: 263, Action: 0, Done: False, Portfolio Value: 10006.496725503966, Reward: 0.0\n",
            "Average batch loss: 41.83960008621216\n",
            "Step: 264, Action: 2, Done: False, Portfolio Value: 9994.590774948796, Reward: 0.0\n",
            "Average batch loss: 35.69637134135701\n",
            "Step: 265, Action: 2, Done: False, Portfolio Value: 9995.667239681403, Reward: 0.0\n",
            "Average batch loss: 38.95138054341078\n",
            "Step: 266, Action: 0, Done: False, Portfolio Value: 9994.942832275185, Reward: 1.8189894035458565e-12\n",
            "Average batch loss: 100.58904081117362\n",
            "Step: 267, Action: 1, Done: False, Portfolio Value: 10002.430683367342, Reward: 0.0\n",
            "Average batch loss: 469.27542250277475\n",
            "Step: 268, Action: 2, Done: False, Portfolio Value: 9999.107677099637, Reward: 0.0\n",
            "Average batch loss: 2368.260906636715\n",
            "Step: 269, Action: 0, Done: False, Portfolio Value: 10007.522102124753, Reward: 0.0\n",
            "Average batch loss: 1667.0220908466727\n",
            "Step: 270, Action: 0, Done: False, Portfolio Value: 9995.755031371167, Reward: -1.8189894035458565e-12\n",
            "Average batch loss: 824.8981114029884\n",
            "Step: 271, Action: 2, Done: False, Portfolio Value: 9994.241741362539, Reward: 0.0\n",
            "Average batch loss: 865.5455444380641\n",
            "Step: 272, Action: 1, Done: False, Portfolio Value: 10001.547555919109, Reward: 0.0\n",
            "Average batch loss: 226.91254558460787\n",
            "Step: 273, Action: 0, Done: False, Portfolio Value: 9994.305821432754, Reward: -1.8189894035458565e-12\n",
            "Average batch loss: 94.2699613776058\n",
            "Step: 274, Action: 1, Done: False, Portfolio Value: 9994.52312941797, Reward: 0.0\n",
            "Average batch loss: 49.30333470724872\n",
            "Step: 275, Action: 2, Done: False, Portfolio Value: 9996.174101041013, Reward: 0.0\n",
            "Average batch loss: 27.67356549948454\n",
            "Step: 276, Action: 1, Done: False, Portfolio Value: 9997.867845982591, Reward: 1.8189894035458565e-12\n",
            "Average batch loss: 5.91219084803015\n",
            "Step: 277, Action: 0, Done: False, Portfolio Value: 9994.9380177553, Reward: 0.0\n",
            "Average batch loss: 3.6766937414067797\n",
            "Step: 278, Action: 0, Done: False, Portfolio Value: 10002.077312629555, Reward: 0.0\n",
            "Average batch loss: 2.591587187276076\n",
            "Step: 279, Action: 2, Done: False, Portfolio Value: 9999.521133236129, Reward: 0.0\n",
            "Average batch loss: 0.5264913937135134\n",
            "Step: 280, Action: 0, Done: False, Portfolio Value: 9994.429774111026, Reward: 1.8189894035458565e-12\n",
            "Average batch loss: 0.2041853901464492\n",
            "Step: 281, Action: 1, Done: False, Portfolio Value: 9994.352674844104, Reward: 0.0\n",
            "Average batch loss: 0.1875727938640921\n",
            "Step: 282, Action: 0, Done: False, Portfolio Value: 9999.365184389271, Reward: 0.0\n",
            "Average batch loss: 0.23897903294709977\n",
            "Step: 283, Action: 2, Done: False, Portfolio Value: 10014.306490557514, Reward: 0.0\n",
            "Average batch loss: 0.2828008958604187\n",
            "Step: 284, Action: 2, Done: False, Portfolio Value: 9997.699824653582, Reward: 0.0\n",
            "Average batch loss: 0.908334270818159\n",
            "Step: 285, Action: 2, Done: False, Portfolio Value: 10001.068709382655, Reward: 0.0\n",
            "Average batch loss: 0.7211214408962405\n",
            "Step: 286, Action: 2, Done: False, Portfolio Value: 10011.116646839624, Reward: 0.0\n",
            "Average batch loss: 0.23678109310731088\n",
            "Step: 287, Action: 0, Done: False, Portfolio Value: 9997.196617161939, Reward: -1.8189894035458565e-12\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-c7a2424558b5>\u001b[0m in \u001b[0;36m<cell line: 94>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;31m# Train the DQN model with the specified episodes and batch size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m \u001b[0mtrain_dqn_half_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistorical_data_directory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-22-f3d5e8f55093>\u001b[0m in \u001b[0;36mtrain_dqn_half_data\u001b[0;34m(episodes, batch_size, directory)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                     \u001b[0mepisode_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-fa5a6d1c481b>\u001b[0m in \u001b[0;36mreplay\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     59\u001b[0m                 \u001b[0mtarget\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_next\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mtarget_f\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m             \u001b[0mtarget_f\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   2618\u001b[0m                     )\n\u001b[1;32m   2619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2620\u001b[0;31m             data_handler = data_adapter.get_data_handler(\n\u001b[0m\u001b[1;32m   2621\u001b[0m                 \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2622\u001b[0m                 \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/data_adapter.py\u001b[0m in \u001b[0;36mget_data_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1686\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_ClusterCoordinatorExactEvalDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ClusterCoordinatorDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute, pss_evaluation_shards)\u001b[0m\n\u001b[1;32m   1290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1291\u001b[0m         \u001b[0madapter_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1292\u001b[0;31m         self._adapter = adapter_cls(\n\u001b[0m\u001b[1;32m   1293\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0mindices_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslice_batch_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m         \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslice_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"batch\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/data_adapter.py\u001b[0m in \u001b[0;36mslice_inputs\u001b[0;34m(self, indices_dataset, inputs)\u001b[0m\n\u001b[1;32m    386\u001b[0m         \"\"\"\n\u001b[1;32m    387\u001b[0m         dataset = tf.data.Dataset.zip(\n\u001b[0;32m--> 388\u001b[0;31m             \u001b[0;34m(\u001b[0m\u001b[0mindices_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m         )\n\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mfrom_tensors\u001b[0;34m(tensors, name)\u001b[0m\n\u001b[1;32m    738\u001b[0m     \u001b[0;31m# pylint: disable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfrom_tensors_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrom_tensors_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m     \u001b[0;31m# pylint: enable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/from_tensors_op.py\u001b[0m in \u001b[0;36m_from_tensors\u001b[0;34m(tensors, name)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_from_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=unused-private-name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_TensorDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/from_tensors_op.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, element, name)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     variant_tensor = gen_dataset_ops.tensor_dataset(\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0moutput_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_flat_tensor_shapes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_structure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36mtensor_dataset\u001b[0;34m(components, output_shapes, metadata, name)\u001b[0m\n\u001b[1;32m   7627\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7628\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7629\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m   7630\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"TensorDataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomponents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"output_shapes\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7631\u001b[0m         output_shapes, \"metadata\", metadata)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training model on only AAPL data for testing (Testing Model)\n"
      ],
      "metadata": {
        "id": "O4bXB6FplaUK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# import os\n",
        "\n",
        "# def load_and_reduce_aapl_data(directory):\n",
        "#     \"\"\"Load and reduce the AAPL stock data by half for testing purposes.\"\"\"\n",
        "#     aapl_file = [file for file in os.listdir(directory) if 'AAPL' in file and file.endswith('.csv')]\n",
        "\n",
        "#     if len(aapl_file) != 1:\n",
        "#         raise FileNotFoundError(\"AAPL data not found or multiple files match the pattern.\")\n",
        "\n",
        "#     # Load the data\n",
        "#     aapl_data = pd.read_csv(os.path.join(directory, aapl_file[0]))\n",
        "#     print(f\"Original AAPL data shape: {aapl_data.shape}\")\n",
        "\n",
        "#     # Reduce data size by half\n",
        "#     reduced_data = aapl_data.sample(frac=0.5, random_state=42)\n",
        "#     print(f\"Reduced AAPL data shape: {reduced_data.shape}\")\n",
        "\n",
        "#     # Split the reduced data into training and testing datasets\n",
        "#     return train_test_split(reduced_data, test_size=0.2, shuffle=True)\n"
      ],
      "metadata": {
        "id": "7oQZvskFlgSK"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def train_dqn_on__aapl(episodes, batch_size, directory):\n",
        "#     print(\"Loading reduced AAPL data...\")\n",
        "#     train_data, test_data = load_and_reduce_aapl_data(directory)\n",
        "\n",
        "#     # Initialize environments using only the reduced AAPL data\n",
        "#     train_env = StockTradingEnv(data=train_data)\n",
        "#     val_env = StockTradingEnv(data=test_data)\n",
        "#     state_size = train_env.state_size\n",
        "#     action_size = 3\n",
        "\n",
        "#     dqn = DQN(state_size, action_size)\n",
        "\n",
        "#     total_rewards = []\n",
        "#     episode_losses = []\n",
        "#     best_reward = -float('inf')\n",
        "#     # Use the .h5 extension to save the entire model\n",
        "#     save_path = 'best_dqn_reduced_aapl_model.h5'\n",
        "#     validation_interval = 5  # Validate every 5 episodes\n",
        "\n",
        "#     for episode in range(episodes):\n",
        "#         print(f\"Starting Episode {episode + 1}/{episodes}...\")\n",
        "#         state = train_env.reset()\n",
        "#         state = np.reshape(state, [1, state_size])\n",
        "#         total_reward = 0\n",
        "#         episode_loss = []\n",
        "\n",
        "#         for step in range(100):  # Limit the steps per episode\n",
        "#             action = dqn.act(state)\n",
        "#             next_state, reward, done, info = train_env.step(action)\n",
        "#             next_state = np.reshape(next_state, [1, state_size])\n",
        "\n",
        "#             dqn.remember(state, action, reward, next_state, done)\n",
        "#             state = next_state\n",
        "#             total_reward += reward\n",
        "\n",
        "#             if done:\n",
        "#                 break\n",
        "\n",
        "#             # Perform experience replay if sufficient samples are available\n",
        "#             if len(dqn.memory) > batch_size:\n",
        "#                 loss = dqn.replay(batch_size)\n",
        "#                 if loss is not None:\n",
        "#                     episode_loss.append(loss)\n",
        "\n",
        "#         average_loss = np.mean(episode_loss) if episode_loss else None\n",
        "#         total_rewards.append(total_reward)\n",
        "#         episode_losses.append(average_loss)\n",
        "\n",
        "#         print(f\"Episode {episode + 1}/{episodes} finished with Reward: {total_reward}, \"\n",
        "#               f\"Avg Loss: {average_loss}, Portfolio Value: {info['current_portfolio_value']}\")\n",
        "\n",
        "#         # Perform validation evaluation every `validation_interval` episodes\n",
        "#         if (episode + 1) % validation_interval == 0:\n",
        "#             print(f\"Starting validation for Episode {episode + 1}\")\n",
        "#             val_loss, val_accuracy = validate_dqn(val_env, dqn, batch_size)\n",
        "#             print(f\"Validation Loss after Episode {episode + 1}: {val_loss}, \"\n",
        "#                   f\"Validation Accuracy: {val_accuracy}\")\n",
        "\n",
        "#         # Save the entire model if the current episode reward is greater\n",
        "#         if total_reward > best_reward:\n",
        "#             best_reward = total_reward\n",
        "#             dqn.save(save_path)\n",
        "#             print(f\"New best reduced AAPL model saved with Reward: {best_reward}\")\n",
        "\n",
        "#     # Test set evaluation\n",
        "#     print(\"Evaluating on Test Set\")\n",
        "#     test_loss, test_accuracy = evaluate_on_test_set(test_data, dqn)\n",
        "#     print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")\n",
        "\n",
        "#     print(\"Training completed.\")\n",
        "#     print(f\"Final Avg Reward: {np.mean(total_rewards)}\")\n",
        "#     print(f\"Final Avg Loss: {np.mean([l for l in episode_losses if l is not None])}\")\n",
        "\n",
        "\n",
        "\n",
        "# # def train_dqn_on_aapl(episodes, batch_size, directory):\n",
        "# #     print(\"Loading reduced AAPL data...\")\n",
        "# #     train_data, test_data = load_and_reduce_aapl_data(directory)\n",
        "# #     train_env = StockTradingEnv(data=train_data)\n",
        "# #     val_env = StockTradingEnv(data=test_data)\n",
        "# #     state_size = train_env.state_size\n",
        "# #     action_size = 3\n",
        "# #     dqn = DQN(state_size, action_size)\n",
        "\n",
        "# #     for episode in range(episodes):\n",
        "# #         state = train_env.reset()\n",
        "# #         state = np.reshape(state, [1, state_size])\n",
        "# #         total_reward = 0\n",
        "# #         for step in range(100):  # Limit the number of steps per episode for debugging\n",
        "# #             action = dqn.act(state)\n",
        "# #             next_state, reward, done, info = train_env.step(action)\n",
        "# #             print(f\"Step {step}: Reward={reward}, Done={done}\")\n",
        "# #             if done:\n",
        "# #                 break\n",
        "# #             state = np.reshape(next_state, [1, state_size])\n",
        "# #             total_reward += reward\n",
        "\n",
        "# #         print(f\"Episode {episode + 1} finished. Total Reward: {total_reward}\")\n"
      ],
      "metadata": {
        "id": "Rt1sOc0olkRZ"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.model_selection import train_test_split\n",
        "# # Set your directory path for the historical data\n",
        "# historical_data_directory = '/content/drive/My Drive/historical_data'\n",
        "\n",
        "# # Number of episodes and batch size for training\n",
        "# episodes = 50  # For initial testing, adjust as needed\n",
        "# batch_size = 32\n",
        "\n",
        "# # Call the function to train the model on AAPL stock data\n",
        "# train_dqn_on__aapl(episodes, batch_size, historical_data_directory)\n"
      ],
      "metadata": {
        "id": "67KNLeWvmRwi"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Testing"
      ],
      "metadata": {
        "id": "T80rndgg-SWq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import models\n",
        "\n",
        "dqn_model = models.load_model('/content/best_dqn_reduced_aapl_model.h5', compile=False)\n",
        "\n",
        "# Manually compile the model\n",
        "dqn_model.compile(\n",
        "    optimizer=optimizers.Adam(learning_rate=0.001),\n",
        "    loss='mse'\n",
        ")"
      ],
      "metadata": {
        "id": "1w7yNxE1YfA0"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = np.array([[50.0, 5, 10000.0, 15]])  # Example: [current_price, shares_held, cash_in_hand, total_shares_owned]\n",
        "test_data = test_data.reshape(1, -1)  # Ensure it's in a 2D shape, with (batch_size, input_dim)\n",
        "\n",
        "# Make predictions using the loaded model\n",
        "predictions = dqn_model.predict(test_data, verbose=0)\n",
        "\n",
        "# Interpret predictions\n",
        "# Assuming an action space with three actions (0: Buy, 1: Sell, 2: Hold)\n",
        "actions = ['Buy', 'Sell', 'Hold']\n",
        "predicted_action = np.argmax(predictions[0])\n",
        "\n",
        "# Output the prediction\n",
        "print(f\"Predicted Action: {actions[predicted_action]}\")\n",
        "print(f\"Raw Model Output: {predictions}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNu8NPyksTn3",
        "outputId": "c02a887c-1193-46bc-80fd-b46d3018cf0c"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Action: Buy\n",
            "Raw Model Output: [[10.745877   0.867387  -4.8331656]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load your trained model\n",
        "dqn_model = load_model('/content/best_dqn_reduced_aapl_model.h5', compile=False)\n",
        "dqn_model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Read the CSV file\n",
        "csv_path = '/content/drive/MyDrive/AAPL_daily_data.csv'\n",
        "data = pd.read_csv(csv_path)\n",
        "\n",
        "# Specify the feature columns expected by the model\n",
        "required_features = ['Open', 'High', 'Low', 'Close']  # Replace with your actual features\n",
        "data = data[required_features]\n",
        "\n",
        "# Normalize the features (adjust if needed)\n",
        "def normalize_features(df):\n",
        "    return (df - df.min()) / (df.max() - df.min())\n",
        "\n",
        "# Apply normalization to the last row\n",
        "last_row = data.iloc[-1]\n",
        "last_row_normalized = normalize_features(data).iloc[-1].values.reshape(1, -1)\n",
        "\n",
        "# Make a prediction for the latest row\n",
        "prediction = dqn_model.predict(last_row_normalized, verbose=0)\n",
        "\n",
        "# Interpret the prediction\n",
        "actions = ['Buy', 'Sell', 'Hold']\n",
        "predicted_action = np.argmax(prediction)\n",
        "print(f\"Predicted Action: {actions[predicted_action]}\")\n",
        "print(f\"Raw Model Output: {prediction}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCLACPIfssyv",
        "outputId": "43bd48f7-c3b4-4a1b-9072-9ee3be6a80a5"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Action: Sell\n",
            "Raw Model Output: [[-0.1452975   0.12315124 -0.09859727]]\n"
          ]
        }
      ]
    }
  ]
}